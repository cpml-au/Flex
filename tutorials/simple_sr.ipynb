{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3950394e",
   "metadata": {},
   "source": [
    "# A simple symbolic regression example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2e278b",
   "metadata": {},
   "source": [
    "This notebook implements a **symbolic regression pipeline** based on **Genetic Programming (GP)** using the `flex` framework and its custom `GPSymbolicRegressor`.\n",
    "\n",
    "The goal is to discover analytical expressions that best fit a given `PMLB` dataset by:\n",
    "- evolving symbolic expressions;\n",
    "- optimizing embedded numerical constants;\n",
    "- penalizing overly complex expressions;\n",
    "- evaluating performance on training and test datasets.\n",
    "\n",
    "The code supports:\n",
    "- multi-variable regression problems;\n",
    "- automatic constant tuning using gradient-based optimization;\n",
    "- parallel execution using Ray."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d87649c",
   "metadata": {},
   "source": [
    "## Imports and Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49fc0b8",
   "metadata": {},
   "source": [
    "This section imports all the required libraries for:\n",
    "\n",
    "- **Genetic Programming**: `deap.gp`, custom `flex.gp` utilities\n",
    "- **Numerical computation**: `numpy`, `mygrad`\n",
    "- **Optimization**: `pygmo` (for constant fitting)\n",
    "- **Machine Learning utilities**: `scikit-learn`\n",
    "- **Parallelism**: `ray`\n",
    "- **Dataset generation**: custom `generate_dataset` function\n",
    "\n",
    "The code relies on both evolutionary optimization (for structure) and gradient-based optimization (for constants)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07bf41f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import gp\n",
    "\n",
    "from flex.gp import regressor as gps\n",
    "from flex.gp.util import (\n",
    "    detect_nested_trigonometric_functions,\n",
    "    load_config_data,\n",
    "    compile_individual_with_consts,\n",
    ")\n",
    "from flex.gp.primitives import add_primitives_to_pset_from_dict\n",
    "import numpy as np\n",
    "import ray\n",
    "\n",
    "\n",
    "import warnings\n",
    "import pygmo as pg\n",
    "\n",
    "import re\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import time\n",
    "\n",
    "import mygrad as mg\n",
    "from mygrad._utils.lock_management import mem_guard_off\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pmlb import fetch_data\n",
    "\n",
    "# set up number of cpus per ray worker\n",
    "num_cpus = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2acdc993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Custom generate dataset function ---\n",
    "def generate_dataset(problem: str=\"1027_ESL\", random_state: int=42, scaleXy: bool=True):\n",
    "    np.random.seed(42)\n",
    "    num_variables = 1\n",
    "    scaler_X = None\n",
    "    scaler_y = None\n",
    "\n",
    "    # PMLB datasets\n",
    "    X, y = fetch_data(problem, return_X_y=True, local_cache_dir=\"./datasets\")\n",
    "\n",
    "    num_variables = X.shape[1]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=random_state\n",
    "    )\n",
    "\n",
    "    y_train = y_train.reshape(-1, 1)\n",
    "    y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "    if scaleXy:\n",
    "        scaler_X = StandardScaler()\n",
    "        scaler_y = StandardScaler()\n",
    "\n",
    "        X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "        y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "        X_test_scaled = scaler_X.transform(X_test)\n",
    "    else:\n",
    "        X_train_scaled = X_train\n",
    "        y_train_scaled = y_train\n",
    "        X_test_scaled = X_test\n",
    "\n",
    "    y_test = y_test.flatten()\n",
    "    y_train_scaled = y_train_scaled.flatten()\n",
    "\n",
    "    num_train_points = X_train.shape[0]\n",
    "\n",
    "\n",
    "    # note y_test and y_train_scaled must be flattened\n",
    "    return (\n",
    "        X_train_scaled,\n",
    "        y_train_scaled,\n",
    "        X_test_scaled,\n",
    "        y_test,\n",
    "        scaler_X,\n",
    "        scaler_y,\n",
    "        num_variables,\n",
    "        num_train_points,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac321d61",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d2a1496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(individual, X, consts=[]):\n",
    "    num_variables = X.shape[1]\n",
    "    if num_variables > 1:\n",
    "        X = [X[:, i] for i in range(num_variables)]\n",
    "    else:\n",
    "        X = [X]\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    y_pred = individual(*X, consts)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def compute_MSE(individual, X, y, consts=[]):\n",
    "    y_pred = eval_model(individual, X, consts)\n",
    "    MSE = np.mean((y - y_pred) ** 2)\n",
    "\n",
    "    if np.isnan(MSE) or np.isinf(MSE):\n",
    "        MSE = 1e8\n",
    "\n",
    "    return MSE\n",
    "\n",
    "\n",
    "def eval_MSE_and_tune_constants(tree, toolbox, X, y):\n",
    "    individual, num_consts = compile_individual_with_consts(tree, toolbox)\n",
    "\n",
    "    if num_consts > 0:\n",
    "\n",
    "        eval_MSE = partial(compute_MSE, individual=individual, X=X, y=y)\n",
    "\n",
    "        x0 = np.ones(num_consts)\n",
    "\n",
    "        class fitting_problem:\n",
    "            def fitness(self, x):\n",
    "                total_err = eval_MSE(consts=x)\n",
    "                # return [total_err + 0.*(np.linalg.norm(x, 2))**2]\n",
    "                return [total_err]\n",
    "\n",
    "            def gradient(self, x):\n",
    "                with mem_guard_off:\n",
    "                    xt = mg.tensor(x, copy=False)\n",
    "                    f = self.fitness(xt)[0]\n",
    "                    f.backward()\n",
    "                return xt.grad\n",
    "\n",
    "            def get_bounds(self):\n",
    "                return (-5.0 * np.ones(num_consts), 5.0 * np.ones(num_consts))\n",
    "\n",
    "        # PYGMO SOLVER\n",
    "        prb = pg.problem(fitting_problem())\n",
    "        algo = pg.algorithm(pg.nlopt(solver=\"lbfgs\"))\n",
    "        # algo = pg.algorithm(pg.pso(gen=10))\n",
    "        # pop = pg.population(prb, size=70)\n",
    "        algo.extract(pg.nlopt).maxeval = 10\n",
    "        pop = pg.population(prb, size=1)\n",
    "        pop.push_back(x0)\n",
    "        pop = algo.evolve(pop)\n",
    "        MSE = pop.champion_f[0]\n",
    "        consts = pop.champion_x\n",
    "\n",
    "        if np.isinf(MSE) or np.isnan(MSE):\n",
    "            MSE = 1e8\n",
    "    else:\n",
    "        MSE = compute_MSE(individual, X, y)\n",
    "        consts = []\n",
    "    return MSE, consts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5d6075",
   "metadata": {},
   "source": [
    "## Expression Complexity and Structure Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2fe80d",
   "metadata": {},
   "source": [
    "These helper functions analyze individuals to:\n",
    "- count trigonometric functions;\n",
    "- detect nested trigonometric expressions;\n",
    "- penalize overly complex or pathological solutions.\n",
    "\n",
    "They are later used to regularize the fitness function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0da61564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_trig_fn(ind):\n",
    "    return len(re.findall(\"cos\", str(ind))) + len(re.findall(\"sin\", str(ind)))\n",
    "\n",
    "\n",
    "def check_nested_trig_fn(ind):\n",
    "    return detect_nested_trigonometric_functions(str(ind))\n",
    "\n",
    "def get_features_batch(\n",
    "    individuals_batch,\n",
    "    individ_feature_extractors=[len, check_nested_trig_fn, check_trig_fn],\n",
    "):\n",
    "    features_batch = [\n",
    "        [fe(i) for i in individuals_batch] for fe in individ_feature_extractors\n",
    "    ]\n",
    "\n",
    "    individ_length = features_batch[0]\n",
    "    nested_trigs = features_batch[1]\n",
    "    num_trigs = features_batch[2]\n",
    "    return individ_length, nested_trigs, num_trigs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91b01b2",
   "metadata": {},
   "source": [
    "## Fitness and Prediction Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76e3097",
   "metadata": {},
   "source": [
    "The fitness function combines:\n",
    "- prediction error (MSE),\n",
    "- structural penalties (expression length),\n",
    "- functional penalties (nested trigonometric functions).\n",
    "\n",
    "A **Tarpeian selection** strategy is applied to discard very large trees early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3116cafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(individuals_batch, toolbox, X, penalty, fitness_scale):\n",
    "\n",
    "    predictions = [None] * len(individuals_batch)\n",
    "\n",
    "    for i, tree in enumerate(individuals_batch):\n",
    "        callable, _ = compile_individual_with_consts(tree, toolbox)\n",
    "        predictions[i] = eval_model(callable, X, consts=tree.consts)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def compute_MSEs(individuals_batch, toolbox, X, y, penalty, fitness_scale):\n",
    "\n",
    "    total_errs = [None] * len(individuals_batch)\n",
    "\n",
    "    for i, tree in enumerate(individuals_batch):\n",
    "        callable, _ = compile_individual_with_consts(tree, toolbox)\n",
    "        total_errs[i] = compute_MSE(callable, X, y, consts=tree.consts)\n",
    "\n",
    "    return total_errs\n",
    "\n",
    "\n",
    "def compute_attributes(individuals_batch, toolbox, X, y, penalty, fitness_scale):\n",
    "\n",
    "    attributes = [None] * len(individuals_batch)\n",
    "\n",
    "    individ_length, nested_trigs, num_trigs = get_features_batch(individuals_batch)\n",
    "\n",
    "    for i, tree in enumerate(individuals_batch):\n",
    "\n",
    "        # Tarpeian selection\n",
    "        if individ_length[i] >= 50:\n",
    "            consts = None\n",
    "            fitness = (1e8,)\n",
    "        else:\n",
    "            MSE, consts = eval_MSE_and_tune_constants(tree, toolbox, X, y)\n",
    "            fitness = (\n",
    "                fitness_scale\n",
    "                * (\n",
    "                    MSE\n",
    "                    + 100000 * nested_trigs[i]\n",
    "                    + 0.0 * num_trigs[i]\n",
    "                    + penalty[\"reg_param\"] * individ_length[i]\n",
    "                ),\n",
    "            )\n",
    "        attributes[i] = {\"consts\": consts, \"fitness\": fitness}\n",
    "    return attributes\n",
    "\n",
    "\n",
    "def assign_attributes(individuals_batch, attributes):\n",
    "    for ind, attr in zip(individuals_batch, attributes):\n",
    "        ind.consts = attr[\"consts\"]\n",
    "        ind.fitness.values = attr[\"fitness\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f33ba80",
   "metadata": {},
   "source": [
    "## Main Training and Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd3c2af",
   "metadata": {},
   "source": [
    "The `eval` function orchestrates the entire symbolic regression process:\n",
    "1. loads configuration from a YAML file;\n",
    "2. generates training and test datasets;\n",
    "3. builds the GP primitive set;\n",
    "4. initializes the symbolic regressor;\n",
    "5. trains the model;\n",
    "6. evaluates performance on training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ada47da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(problem, cfgfile, seed=42):\n",
    "\n",
    "    regressor_params, config_file_data = load_config_data(cfgfile)\n",
    "\n",
    "    scaleXy = config_file_data[\"gp\"][\"scaleXy\"]\n",
    "\n",
    "    # generate training and test datasets\n",
    "    (\n",
    "        X_train_scaled,\n",
    "        y_train_scaled,\n",
    "        X_test_scaled,\n",
    "        y_test,\n",
    "        _,\n",
    "        scaler_y,\n",
    "        num_variables,\n",
    "        _,\n",
    "    ) = generate_dataset(problem, scaleXy=scaleXy, random_state=seed)\n",
    "\n",
    "    if num_variables == 1:\n",
    "        pset = gp.PrimitiveSetTyped(\"Main\", [float], float)\n",
    "        pset.renameArguments(ARG0=\"x\")\n",
    "    elif num_variables == 2:\n",
    "        pset = gp.PrimitiveSetTyped(\"Main\", [float, float], float)\n",
    "        pset.renameArguments(ARG0=\"x\")\n",
    "        pset.renameArguments(ARG1=\"y\")\n",
    "    else:\n",
    "        pset = gp.PrimitiveSetTyped(\"Main\", [float] * num_variables, float)\n",
    "\n",
    "    pset = add_primitives_to_pset_from_dict(pset, config_file_data[\"gp\"][\"primitives\"])\n",
    "\n",
    "    batch_size = config_file_data[\"gp\"][\"batch_size\"]\n",
    "    if config_file_data[\"gp\"][\"use_constants\"]:\n",
    "        pset.addTerminal(object, float, \"c\")\n",
    "\n",
    "    callback_func = assign_attributes\n",
    "    fitness_scale = 1.0\n",
    "\n",
    "    penalty = config_file_data[\"gp\"][\"penalty\"]\n",
    "    common_params = {\"penalty\": penalty, \"fitness_scale\": fitness_scale}\n",
    "\n",
    "    gpsr = gps.GPSymbolicRegressor(\n",
    "        pset_config=pset,\n",
    "        fitness=compute_attributes,\n",
    "        predict_func=predict,\n",
    "        score_func=compute_MSEs,\n",
    "        common_data=common_params,\n",
    "        callback_func=callback_func,\n",
    "        print_log=True,\n",
    "        num_best_inds_str=1,\n",
    "        save_best_individual=False,\n",
    "        output_path=\"./\",\n",
    "        seed_str=None,\n",
    "        batch_size=batch_size,\n",
    "        num_cpus=num_cpus,\n",
    "        remove_init_duplicates=True,\n",
    "        **regressor_params,\n",
    "    )\n",
    "\n",
    "    \n",
    "    est = gpsr\n",
    "\n",
    "    tic = time.time()\n",
    "    est.fit(X_train_scaled, y_train_scaled)\n",
    "    toc = time.time()\n",
    "\n",
    "    best = est.get_best_individuals(n_ind=1)[0]\n",
    "\n",
    "    if hasattr(best, \"consts\"):\n",
    "        print(\"Best parameters = \", best.consts)\n",
    "\n",
    "    print(\"Elapsed time = \", toc - tic)\n",
    "    individuals_per_sec = (\n",
    "        (est.get_last_gen() + 1)\n",
    "        * gpsr.num_individuals\n",
    "        * gpsr.num_islands\n",
    "        / (toc - tic)\n",
    "    )\n",
    "    print(\"Individuals per sec = \", individuals_per_sec)\n",
    "\n",
    "    u_best = est.predict(X_test_scaled)\n",
    "\n",
    "    # de-scale outputs before computing errors\n",
    "    if scaleXy:\n",
    "        u_best = scaler_y.inverse_transform(u_best.reshape(-1, 1)).flatten()\n",
    "\n",
    "    MSE = np.mean((u_best - y_test) ** 2)\n",
    "    r2_test = r2_score(y_test, u_best)\n",
    "    print(\"MSE on the test set = \", MSE)\n",
    "    print(\"R^2 on the test set = \", r2_test)\n",
    "\n",
    "    pred_train = est.predict(X_train_scaled)\n",
    "\n",
    "    if scaleXy:\n",
    "        pred_train = scaler_y.inverse_transform(pred_train.reshape(-1, 1)).flatten()\n",
    "        y_train_scaled = scaler_y.inverse_transform(\n",
    "            y_train_scaled.reshape(-1, 1)\n",
    "        ).flatten()\n",
    "\n",
    "    MSE = np.mean((pred_train - y_train_scaled) ** 2)\n",
    "    r2_train = r2_score(y_train_scaled, pred_train)\n",
    "    print(\"MSE on the training set = \", MSE)\n",
    "    print(\"R^2 on the training set = \", r2_train)\n",
    "\n",
    "    return r2_train, r2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99061dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEM:  1027_ESL\n",
      "seed:  29802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 15:07:11,631\tINFO worker.py:2007 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating initial population(s)...\n",
      " Removing duplicates from initial population(s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smanti/mambaforge/envs/flex_test/lib/python3.12/site-packages/ray/_private/worker.py:2046: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DONE.\n",
      "DONE.\n",
      "Evaluating initial population(s)...\n",
      "DONE.\n",
      " -= START OF EVOLUTION =- \n",
      "   \t     \t           fitness            \t             size             \n",
      "   \t     \t------------------------------\t------------------------------\n",
      "gen\tevals\tmin   \tavg   \tmax   \tstd   \tmin\tavg   \tmax\tstd   \n",
      "1  \t2000 \t0.1663\t0.6675\t1.1616\t0.2429\t2  \t8.0545\t21 \t3.8996\n",
      "Best individuals of this generation:\n",
      "aq(add(add(ARG0, ARG2), ARG3), c)\n",
      "2  \t2000 \t0.1663\t0.4828\t0.6572\t0.0924\t2  \t7.7195\t22 \t3.9847\n",
      "Best individuals of this generation:\n",
      "aq(add(add(ARG0, ARG2), ARG3), c)\n",
      "3  \t2000 \t0.1663\t0.4286\t0.5213\t0.0732\t2  \t7.942 \t25 \t4.1436\n",
      "Best individuals of this generation:\n",
      "aq(add(add(ARG0, ARG2), ARG3), c)\n",
      "4  \t2000 \t0.1617\t0.389 \t0.4812\t0.0637\t2  \t8.824 \t28 \t4.4514\n",
      "Best individuals of this generation:\n",
      "mul(c, add(add(ARG2, ARG3), add(ARG1, ARG0)))\n",
      "5  \t2000 \t0.1617\t0.3546\t0.4292\t0.0545\t2  \t9.465 \t28 \t4.8756\n",
      "Best individuals of this generation:\n",
      "mul(c, add(add(ARG2, ARG3), add(ARG1, ARG0)))\n",
      "6  \t2000 \t0.1617\t0.3282\t0.3986\t0.0469\t3  \t10.401\t31 \t5.0057\n",
      "Best individuals of this generation:\n",
      "mul(c, add(add(ARG2, ARG3), add(ARG1, ARG0)))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=gcs_server)\u001b[0m [2026-01-13 15:07:38,404 E 2414 2414] (gcs_server) gcs_server.cc:303: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7  \t2000 \t0.1617\t0.3078\t0.3661\t0.043 \t3  \t11.3225\t32 \t5.2708\n",
      "Best individuals of this generation:\n",
      "mul(c, add(add(ARG2, ARG3), add(ARG1, ARG0)))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2026-01-13 15:07:40,702 E 2562 2562] (raylet) main.cc:1032: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8  \t2000 \t0.1617\t0.2892\t0.3431\t0.0391\t3  \t12.388 \t33 \t5.5686\n",
      "Best individuals of this generation:\n",
      "mul(c, add(add(ARG2, ARG3), add(ARG1, ARG0)))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(compute_attributes pid=2726)\u001b[0m [2026-01-13 15:07:42,777 E 2726 2851] core_worker_process.cc:842: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "[2026-01-13 15:07:43,034 E 2361 2722] core_worker_process.cc:842: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9  \t2000 \t0.1511\t0.273 \t0.3322\t0.0355\t5  \t13.485 \t39 \t5.4463\n",
      "Best individuals of this generation:\n",
      "mul(c, add(ARG3, add(add(ARG0, ARG2), add(ARG1, ARG3))))\n",
      "10 \t2000 \t0.1511\t0.2572\t0.3142\t0.0305\t5  \t13.8875\t39 \t5.6526\n",
      "Best individuals of this generation:\n",
      "mul(c, add(ARG3, add(add(ARG0, ARG2), add(ARG1, ARG3))))\n",
      "11 \t2000 \t0.1511\t0.245 \t0.287 \t0.0266\t5  \t13.785 \t36 \t5.5292\n",
      "Best individuals of this generation:\n",
      "mul(c, add(ARG3, add(add(ARG0, ARG2), add(ARG1, ARG3))))\n",
      "12 \t2000 \t0.1511\t0.2354\t0.2715\t0.0245\t5  \t13.875 \t35 \t5.4484\n",
      "Best individuals of this generation:\n",
      "mul(c, add(ARG3, add(add(ARG0, ARG2), add(ARG1, ARG3))))\n",
      "13 \t2000 \t0.1511\t0.2256\t0.2577\t0.0225\t5  \t14.247 \t34 \t5.6068\n",
      "Best individuals of this generation:\n",
      "mul(c, add(ARG3, add(add(ARG0, ARG2), add(ARG1, ARG3))))\n",
      "14 \t2000 \t0.1511\t0.2168\t0.2453\t0.0203\t5  \t14.9135\t38 \t5.7694\n",
      "Best individuals of this generation:\n",
      "mul(c, add(ARG3, add(add(ARG0, ARG2), add(ARG1, ARG3))))\n",
      "15 \t2000 \t0.1511\t0.2088\t0.2325\t0.0186\t5  \t15.1545\t44 \t5.9342\n",
      "Best individuals of this generation:\n",
      "mul(c, add(ARG3, add(add(ARG0, ARG2), add(ARG1, ARG3))))\n",
      "16 \t2000 \t0.1511\t0.2014\t0.2242\t0.0175\t5  \t15.1885\t40 \t6.2492\n",
      "Best individuals of this generation:\n",
      "mul(c, add(ARG3, add(add(ARG0, ARG2), add(ARG1, ARG3))))\n",
      "17 \t2000 \t0.1511\t0.1946\t0.2162\t0.0158\t5  \t15.8435\t40 \t6.2322\n",
      "Best individuals of this generation:\n",
      "mul(c, add(ARG3, add(add(ARG0, ARG2), add(ARG1, ARG3))))\n",
      "18 \t2000 \t0.1463\t0.188 \t0.2093\t0.0135\t5  \t16.687 \t44 \t6.4215\n",
      "Best individuals of this generation:\n",
      "mul(add(aq(add(ARG0, ARG2), mul(ARG1, c)), add(ARG1, ARG3)), c)\n",
      "19 \t2000 \t0.1463\t0.1821\t0.2009\t0.0105\t7  \t17.7125\t40 \t5.9443\n",
      "Best individuals of this generation:\n",
      "mul(add(aq(add(ARG0, ARG2), mul(ARG1, c)), add(ARG1, ARG3)), c)\n",
      "20 \t2000 \t0.1463\t0.1776\t0.1914\t0.0082\t7  \t17.801 \t40 \t5.8378\n",
      "Best individuals of this generation:\n",
      "mul(add(aq(add(ARG0, ARG2), mul(ARG1, c)), add(ARG1, ARG3)), c)\n",
      " -= END OF EVOLUTION =- \n",
      "The best individual is mul(add(aq(add(ARG0, ARG2), mul(ARG1, c)), add(ARG1, ARG3)), c)\n",
      "The best fitness on the training set is 0.1463\n",
      "Best parameters =  [1.4722337 0.3701102]\n",
      "Elapsed time =  67.35689806938171\n",
      "Individuals per sec =  623.5441536624421\n",
      "MSE on the test set =  0.38021101233691773\n",
      "R^2 on the test set =  0.848853911283815\n",
      "MSE on the training set =  0.2452834824837036\n",
      "R^2 on the training set =  0.8666969828606731\n",
      "{'problem': '1027_ESL', 'r2_train': 0.8666969828606731, 'r2_test': 0.848853911283815, 'seed': 29802}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(compute_attributes pid=2737)\u001b[0m [2026-01-13 15:07:42,982 E 2737 3416] core_worker_process.cc:842: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\u001b[32m [repeated 15x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "problem = \"1027_ESL\"\n",
    "cfgfile = \"simple_sr.yaml\"\n",
    "\n",
    "\n",
    "#seeds = [29802, 22118, 860, 15795, 21575, 5390, 11964, 6265, 23654, 11284]\n",
    "seed = 29802\n",
    "\n",
    "r2_tests = []\n",
    "\n",
    "# possibly add rmse_train, rmse_test, rmse_val\n",
    "header = [\"problem\", \"trial\", \"r2_train\", \"r2_test\", \"seed\"]\n",
    "\n",
    "\n",
    "print(\"PROBLEM: \", problem)\n",
    "print(\"seed: \", seed)\n",
    "r2_train, r2_test = eval(problem=problem, cfgfile=cfgfile, seed=seed)\n",
    "r2_tests.append(r2_test)\n",
    "\n",
    "stats = {\n",
    "    \"problem\": problem,\n",
    "    \"r2_train\": r2_train,\n",
    "    \"r2_test\": r2_test,\n",
    "    \"seed\": seed,\n",
    "}\n",
    "\n",
    "print(stats)\n",
    "\n",
    "# ray is explicitly shut down at the end of the execution to release computational resources\n",
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flex_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
